{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Estimate depth using Depth Anything model"
      ],
      "metadata": {
        "id": "y6Lmk-PQpMxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you have git-lfs installed (https://git-lfs.com)\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/spaces/LiheYoung/Depth-Anything\n",
        "%cd Depth-Anything\n",
        "!pip install -r requirements.txt --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnSiZfNC0zzP",
        "outputId": "ea10a4c7-430c-4d85-d695-789a600fb358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Depth-Anything'...\n",
            "remote: Enumerating objects: 393, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 393 (delta 1), reused 0 (delta 0), pack-reused 387 (from 1)\u001b[K\n",
            "Receiving objects: 100% (393/393), 2.25 MiB | 7.51 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n",
            "Filtering content: 100% (84/84), 9.93 GiB | 38.78 MiB/s, done.\n",
            "/content/Depth-Anything\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.1/305.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: typer 0.12.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access Google Drive data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "P8Q9e57bptZA",
        "outputId": "5c0125c8-8265-4cae-85a0-ab85cdb9cd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4cebfc3d8ad7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Access Google Drive data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Relative depth estimation using Depth Anything model"
      ],
      "metadata": {
        "id": "0cbQe9bJqAKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import Compose\n",
        "from tqdm import tqdm\n",
        "\n",
        "SCANNET_SCENE_DIR = '/content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00'\n",
        "\n",
        "from depth_anything.dpt import DepthAnything\n",
        "from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
        "\n",
        "# Possible values for the encoder: ['vits', 'vitb', 'vitl']\n",
        "\n",
        "def estimate_depth(depth_anything_model, img_path, out_dir, out_dir_vis):\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    depth_anything_model = depth_anything_model.to(DEVICE).eval()\n",
        "\n",
        "    total_params = sum(param.numel() for param in depth_anything_model.parameters())\n",
        "    print('Total parameters: {:.2f}M'.format(total_params / 1e6))\n",
        "\n",
        "    transform = Compose([\n",
        "        Resize(\n",
        "            width=518,\n",
        "            height=518,\n",
        "            resize_target=False,\n",
        "            keep_aspect_ratio=True,\n",
        "            ensure_multiple_of=14,\n",
        "            resize_method='lower_bound',\n",
        "            image_interpolation_method=cv2.INTER_CUBIC,\n",
        "        ),\n",
        "        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        PrepareForNet(),\n",
        "    ])\n",
        "\n",
        "    if os.path.isfile(img_path):\n",
        "        if img_path.endswith('txt'):\n",
        "            with open(img_path, 'r') as f:\n",
        "                filenames = f.read().splitlines()\n",
        "        else:\n",
        "            filenames = [img_path]\n",
        "    else:\n",
        "        filenames = os.listdir(img_path)\n",
        "        filenames = [os.path.join(img_path, filename) for filename in filenames if not filename.startswith('.')]\n",
        "        filenames.sort()\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    for filename in tqdm(filenames):\n",
        "        raw_image = cv2.imread(filename)\n",
        "        image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB) / 255.0\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        image = transform({'image': image})['image']\n",
        "        image = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            depth = depth_anything_model(image)\n",
        "\n",
        "        depth = F.interpolate(depth[None], (h, w), mode='bilinear', align_corners=False)[0, 0]\n",
        "        depth = depth.cpu().numpy()\n",
        "\n",
        "        # Save result\n",
        "        filename = os.path.basename(filename)\n",
        "        if not os.path.exists(out_dir):\n",
        "            os.makedirs(out_dir)\n",
        "        np.save(os.path.join(out_dir, filename[:filename.rfind('.')] + '.npy'), depth)\n",
        "\n",
        "        # Save some visualizations\n",
        "        if (out_dir_vis is not None):\n",
        "            if not os.path.exists(out_dir_vis):\n",
        "                os.makedirs(out_dir_vis)\n",
        "\n",
        "            depth = ((depth - depth.min()) / (depth.max() - depth.min()) * 255.0).astype(np.uint8)\n",
        "\n",
        "            depth_gray = np.repeat(depth[..., np.newaxis], 3, axis=-1)\n",
        "            cv2.imwrite(os.path.join(out_dir_vis, filename[:filename.rfind('.')] + '_gray.png'), depth_gray)\n",
        "\n",
        "            depth_false_color = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)\n",
        "            cv2.imwrite(os.path.join(out_dir_vis, filename[:filename.rfind('.')] + '_color.png'), depth_false_color)\n",
        "    return"
      ],
      "metadata": {
        "id": "LAtwCLV4fEXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xf2IQC0o3wp"
      },
      "outputs": [],
      "source": [
        "# Define Depth Anything model using Visual Transformer Large (vitl)\n",
        "depth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_vitl14')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = os.path.join(SCANNET_SCENE_DIR, 'test/rgb')\n",
        "out_dir = os.path.join(SCANNET_SCENE_DIR, 'test/depth_DA')\n",
        "out_dir_vis = os.path.join(SCANNET_SCENE_DIR, 'test/depth_DA_visualization')\n",
        "estimate_depth(depth_anything, img_path, out_dir, out_dir_vis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1VmwDHTieEU",
        "outputId": "18b526f4-cd90-4534-e8b2-08cdc27381cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 335.32M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [06:16<00:00, 47.05s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = os.path.join(SCANNET_SCENE_DIR, 'train/rgb')\n",
        "out_dir = os.path.join(SCANNET_SCENE_DIR, 'train/depth_DA')\n",
        "out_dir_vis = os.path.join(SCANNET_SCENE_DIR, 'train/depth_DA_visualization')\n",
        "estimate_depth(depth_anything, img_path, out_dir, out_dir_vis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "-nuPm37EkpVy",
        "outputId": "3694593c-ef03-454f-d5e6-11b22abc8701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-21ee0ec0843a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCANNET_SCENE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train/rgb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCANNET_SCENE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train/depth_DA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mout_dir_vis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCANNET_SCENE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train/depth_DA_visualization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mestimate_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_anything\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir_vis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Metric depth estimation using fine tuned Depth Anything model"
      ],
      "metadata": {
        "id": "ZlT37va0ks4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Depth Anything from GitHub to have the ZoeDepth requirements\n",
        "%mkdir Depth_Anything_GitHub`\n",
        "%cd Depth_Anything_GitHub\n",
        "!git clone https://github.com/LiheYoung/Depth-Anything\n",
        "%cd Depth-Anything\n",
        "!pip install -r requirements.txt --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thfMp08fRWSq",
        "outputId": "6eed6e70-aa8c-49f8-db13-71158d5e0524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: unexpected EOF while looking for matching ``'\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n",
            "[Errno 2] No such file or directory: 'Depth_Anything_GitHub'\n",
            "/content/Depth-Anything\n",
            "Cloning into 'Depth-Anything'...\n",
            "remote: Enumerating objects: 430, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 430 (delta 107), reused 47 (delta 41), pack-reused 277\u001b[K\n",
            "Receiving objects: 100% (430/430), 237.89 MiB | 33.37 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n",
            "Updating files: 100% (219/219), done.\n",
            "/content/Depth-Anything/Depth-Anything\n",
            "\u001b[33mWARNING: typer 0.12.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd metric_depth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9UNbJPeWk-T",
        "outputId": "f4df6c36-50a1-4d02-c355-847d85ffe250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Depth-Anything/Depth-Anything/metric_depth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "with open(\"environment.yml\") as file_handle:\n",
        "    environment_data = yaml.safe_load(file_handle)\n",
        "\n",
        "for dependency in environment_data[\"dependencies\"]:\n",
        "    if isinstance(dependency, dict):\n",
        "      for lib in dependency['pip']:\n",
        "        os.system(f\"pip install {lib}\")"
      ],
      "metadata": {
        "id": "3omlx8xKibHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy Depth Anything checkpoints so that zoedepth can find them\n",
        "%cp -r /content/Depth-Anything/checkpoints ./checkpoints"
      ],
      "metadata": {
        "id": "r6bV-gHcbNS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "IHCaochtCvqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Depth Anything model using Visual Transformer Large (vitl) fine tuned on NYU_v2\n",
        "# Code based on script by @1ssb\n",
        "# https://github.com/LiheYoung/Depth-Anything/issues/36\n",
        "\n",
        "# import argparse\n",
        "from tqdm import tqdm\n",
        "import os, glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import Compose\n",
        "import torchvision.transforms as transforms\n",
        "from zoedepth.models.builder import build_model\n",
        "from zoedepth.utils.config import get_config\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def infer(model, image, dataset):\n",
        "    \"\"\"\n",
        "    Performs model inference on a single image.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The depth estimation model.\n",
        "        image (torch.Tensor): The input image tensor.\n",
        "        dataset (str): The name of the dataset being used.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Predicted depth map.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    pred = model(image, dataset=dataset)\n",
        "    return pred\n",
        "\n",
        "def get_depth_from_prediction(pred):\n",
        "    \"\"\"\n",
        "    Extracts the depth map from model prediction.\n",
        "\n",
        "    Args:\n",
        "        pred (torch.Tensor | list | tuple | dict): Model prediction.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Extracted depth map.\n",
        "    \"\"\"\n",
        "    if isinstance(pred, torch.Tensor):\n",
        "        return pred\n",
        "    elif isinstance(pred, (list, tuple)):\n",
        "        return pred[-1]\n",
        "    elif isinstance(pred, dict):\n",
        "        return pred.get('metric_depth', pred.get('out'))\n",
        "    else:\n",
        "        raise TypeError(f\"Unknown output type {type(pred)}\")\n",
        "\n",
        "def process_image(model, image_path, out_dir, out_dir_vis, dataset):\n",
        "    \"\"\"\n",
        "    Processes a single image, performs depth estimation, and saves the resulting point cloud.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The depth estimation model.\n",
        "        image_path (str): Path to the image file.\n",
        "        out_dir (str): Directory to save the predicted depth.\n",
        "        out_dir_vis (str): Directory to save the visualization.\n",
        "        dataset (str): The name of the dataset being used.\n",
        "    \"\"\"\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    transform = Compose([\n",
        "        Resize(\n",
        "            width=392,\n",
        "            height=518,\n",
        "            resize_target=False,\n",
        "            keep_aspect_ratio=True,\n",
        "            ensure_multiple_of=14,\n",
        "            resize_method='lower_bound',\n",
        "            image_interpolation_method=cv2.INTER_CUBIC,\n",
        "        ),\n",
        "        #NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        PrepareForNet(),\n",
        "    ])\n",
        "\n",
        "    raw_image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB) / 255.0\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    image = transform({'image': image})['image']\n",
        "    image = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    pred_dict = infer(model, image, dataset)\n",
        "    pred = get_depth_from_prediction(pred_dict).squeeze(0)\n",
        "\n",
        "    depth = F.interpolate(pred[None], (h, w), mode='bilinear', align_corners=False)[0, 0]\n",
        "    depth = pred.squeeze().detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "    min_depth, max_depth = np.min(depth[depth > 0]), np.max(depth)\n",
        "    print(f\"Processed {image_path}: Min Depth: {min_depth}, Max Depth: {max_depth}\")\n",
        "\n",
        "    # Save result\n",
        "    filename = os.path.basename(image_path)\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "    np.save(os.path.join(out_dir, filename[:filename.rfind('.')] + '.npy'), depth)\n",
        "\n",
        "    # Save some visualizations\n",
        "    if (out_dir_vis is not None):\n",
        "        if not os.path.exists(out_dir_vis):\n",
        "            os.makedirs(out_dir_vis)\n",
        "\n",
        "        depth = ((depth - depth.min()) / (depth.max() - depth.min()) * 255.0).astype(np.uint8)\n",
        "\n",
        "        depth_gray = np.repeat(depth[..., np.newaxis], 3, axis=-1)\n",
        "        cv2.imwrite(os.path.join(out_dir_vis, filename[:filename.rfind('.')] + '_gray.png'), depth_gray)\n",
        "\n",
        "        depth_false_color = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)\n",
        "        cv2.imwrite(os.path.join(out_dir_vis, filename[:filename.rfind('.')] + '_color.png'), depth_false_color)\n",
        "    return\n",
        "\n",
        "def main(config, input_dir, output_dir, out_dir_vis, dataset):\n",
        "    \"\"\"\n",
        "    Main function to process all images in a directory.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Configuration for the model.\n",
        "        input_dir (str): Directory containing input images.\n",
        "        output_dir (str): Directory to save point clouds.\n",
        "        dataset (str): The name of the dataset being used.\n",
        "    \"\"\"\n",
        "    model = build_model(config).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    image_paths = glob.glob(os.path.join(input_dir, '*.png')) + glob.glob(os.path.join(input_dir, '*.jpg'))\n",
        "    if not image_paths:\n",
        "        print(\"No images found in the input directory.\")\n",
        "        return\n",
        "\n",
        "    for image_path in tqdm(image_paths, desc=\"Processing Images\"):\n",
        "        try:\n",
        "            process_image(model, image_path, output_dir, out_dir_vis, dataset)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {e}\")\n",
        "\n",
        "def test_model(model_name, pretrained_resource, input_dir, output_dir, out_dir_vis, dataset):\n",
        "    \"\"\"\n",
        "    Tests a model with given parameters.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the model.\n",
        "        pretrained_resource (str): Path to pretrained model weights.\n",
        "        input_dir (str): Directory containing input images.\n",
        "        output_dir (str): Directory to save point clouds.\n",
        "        dataset (str): The name of the dataset being used.\n",
        "    \"\"\"\n",
        "    config = get_config(model_name, \"eval\", dataset)\n",
        "    if pretrained_resource:\n",
        "        config.pretrained_resource = pretrained_resource\n",
        "    main(config, input_dir, output_dir, out_dir_vis, dataset)"
      ],
      "metadata": {
        "id": "YrxLlLFzbk3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read camera parameters for the sequence\n",
        "# Not used right now but might be interesting in the future\n",
        "def read_camera_list(filename):\n",
        "  \"\"\"\n",
        "  Reads camera information from a text file in COLMAP format.\n",
        "\n",
        "  Args:\n",
        "    filename: The path to the text file.\n",
        "\n",
        "  Returns:\n",
        "    A list of dictionaries, where each dictionary represents a camera with keys:\n",
        "      - camera_id: The ID of the camera (string).\n",
        "      - model: The camera model (string).\n",
        "      - width: The image width (integer).\n",
        "      - height: The image height (integer).\n",
        "      - params: A list of camera parameters (floats).\n",
        "  \"\"\"\n",
        "  with open(filename, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "    # Find the number of cameras from the header\n",
        "    for line in lines:\n",
        "        if not line.startswith('#'):\n",
        "          # Read camera information\n",
        "          camera_data = line.strip().split()\n",
        "\n",
        "          # Extract data\n",
        "          camera_id, model, width, height, *params = camera_data\n",
        "\n",
        "          # Convert data types\n",
        "          width = int(width)\n",
        "          height = int(height)\n",
        "          params = [float(p) for p in params]\n",
        "\n",
        "          # Create camera dictionary\n",
        "          return {\n",
        "              \"camera_id\": camera_id,\n",
        "              \"model\": model,\n",
        "              \"width\": width,\n",
        "              \"height\": height,\n",
        "              \"focal_length\": params[0],\n",
        "              \"central_point_x\": params[1],\n",
        "              \"central_point_y\": params[2],\n",
        "          }"
      ],
      "metadata": {
        "id": "RUx2dOc954fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = os.path.join(SCANNET_SCENE_DIR, 'test/rgb')\n",
        "out_dir = os.path.join(SCANNET_SCENE_DIR, 'test/metric_depth_DA_ft_NYUv2')\n",
        "out_dir_vis = os.path.join(SCANNET_SCENE_DIR, 'test/metric_depth_DA_ft_NYUv2_visualization')\n",
        "# estimate_depth(depth_anything_fine_tuned_NYU_v2, img_path, out_dir, out_dir_vis)\n",
        "test_model('zoedepth', 'local::/content/Depth-Anything/checkpoints_metric_depth/depth_anything_metric_depth_indoor.pt', img_path, out_dir, out_dir_vis, 'nyu')"
      ],
      "metadata": {
        "id": "w48AGOazk94s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7b98e5-8e6a-432c-9dfb-c1743729581f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params passed to Resize transform:\n",
            "\twidth:  518\n",
            "\theight:  392\n",
            "\tresize_target:  True\n",
            "\tkeep_aspect_ratio:  False\n",
            "\tensure_multiple_of:  14\n",
            "\tresize_method:  minimal\n",
            "Using pretrained resource local::/content/Depth-Anything/checkpoints_metric_depth/depth_anything_metric_depth_indoor.pt\n",
            "Loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Images:  12%|█▎        | 1/8 [00:22<02:35, 22.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/test/rgb/417.jpg: Min Depth: 0.667252242565155, Max Depth: 5.105383396148682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  25%|██▌       | 2/8 [00:48<02:29, 24.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/test/rgb/853.jpg: Min Depth: 0.9710784554481506, Max Depth: 2.736940622329712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  38%|███▊      | 3/8 [01:09<01:54, 22.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/test/rgb/200.jpg: Min Depth: 0.7274956703186035, Max Depth: 2.606996536254883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  50%|█████     | 4/8 [01:29<01:26, 21.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/test/rgb/965.jpg: Min Depth: 1.1321760416030884, Max Depth: 4.995934963226318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  62%|██████▎   | 5/8 [01:48<01:02, 20.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/test/rgb/714.jpg: Min Depth: 0.6265469789505005, Max Depth: 5.582090854644775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  75%|███████▌  | 6/8 [02:09<00:41, 20.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/test/rgb/12.jpg: Min Depth: 0.8831966519355774, Max Depth: 3.143127679824829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  88%|████████▊ | 7/8 [02:28<00:20, 20.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/test/rgb/211.jpg: Min Depth: 0.7460336685180664, Max Depth: 2.4937057495117188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Images: 100%|██████████| 8/8 [02:48<00:00, 21.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/test/rgb/258.jpg: Min Depth: 0.8508521318435669, Max Depth: 3.3603603839874268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = os.path.join(SCANNET_SCENE_DIR, 'train/rgb')\n",
        "out_dir = os.path.join(SCANNET_SCENE_DIR, 'train/metric_depth_DA_ft_NYUv2')\n",
        "out_dir_vis = os.path.join(SCANNET_SCENE_DIR, 'train/metric_depth_DA_ft_NYUv2_visualization')\n",
        "test_model('zoedepth', 'local::/content/Depth-Anything/checkpoints_metric_depth/depth_anything_metric_depth_indoor.pt', img_path, out_dir, out_dir_vis, 'nyu')"
      ],
      "metadata": {
        "id": "K6Y4o6y1k-Wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2950978-920d-4ecd-be5d-17473e556d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params passed to Resize transform:\n",
            "\twidth:  518\n",
            "\theight:  392\n",
            "\tresize_target:  True\n",
            "\tkeep_aspect_ratio:  False\n",
            "\tensure_multiple_of:  14\n",
            "\tresize_method:  minimal\n",
            "Using pretrained resource local::/content/Depth-Anything/checkpoints_metric_depth/depth_anything_metric_depth_indoor.pt\n",
            "Loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:   0%|          | 0/18 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/333.jpg: Min Depth: 0.793071448802948, Max Depth: 6.502934455871582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Images:  11%|█         | 2/18 [00:42<05:39, 21.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/229.jpg: Min Depth: 0.8292563557624817, Max Depth: 2.997804641723633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  17%|█▋        | 3/18 [01:04<05:17, 21.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/86.jpg: Min Depth: 0.8270928263664246, Max Depth: 3.2769672870635986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  22%|██▏       | 4/18 [01:23<04:47, 20.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/347.jpg: Min Depth: 0.752595841884613, Max Depth: 6.0402750968933105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  28%|██▊       | 5/18 [01:46<04:39, 21.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/149.jpg: Min Depth: 0.7656700015068054, Max Depth: 3.0674631595611572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  33%|███▎      | 6/18 [02:06<04:11, 20.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/639.jpg: Min Depth: 1.3340481519699097, Max Depth: 4.455844879150391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  39%|███▉      | 7/18 [02:26<03:46, 20.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/677.jpg: Min Depth: 0.6701034903526306, Max Depth: 6.152810573577881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  44%|████▍     | 8/18 [02:47<03:27, 20.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/794.jpg: Min Depth: 0.8746646046638489, Max Depth: 5.774078845977783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  50%|█████     | 9/18 [03:07<03:04, 20.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/1030.jpg: Min Depth: 0.9427445530891418, Max Depth: 4.111330032348633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  56%|█████▌    | 10/18 [03:29<02:46, 20.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/280.jpg: Min Depth: 0.7710055708885193, Max Depth: 5.116616249084473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  61%|██████    | 11/18 [03:49<02:24, 20.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/320.jpg: Min Depth: 0.7512742280960083, Max Depth: 6.040557861328125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  67%|██████▋   | 12/18 [04:09<02:02, 20.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/951.jpg: Min Depth: 1.2117100954055786, Max Depth: 5.221408367156982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  72%|███████▏  | 13/18 [04:31<01:45, 21.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/448.jpg: Min Depth: 0.7787032723426819, Max Depth: 4.8587541580200195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  78%|███████▊  | 14/18 [04:51<01:22, 20.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/930.jpg: Min Depth: 1.1113507747650146, Max Depth: 4.527143955230713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  83%|████████▎ | 15/18 [05:12<01:02, 20.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/759.jpg: Min Depth: 0.6677700877189636, Max Depth: 6.23529052734375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  89%|████████▉ | 16/18 [05:32<00:41, 20.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/610.jpg: Min Depth: 1.3050090074539185, Max Depth: 3.6760177612304688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Images:  94%|█████████▍| 17/18 [05:53<00:20, 20.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/979.jpg: Min Depth: 0.9364773631095886, Max Depth: 4.350281715393066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Images: 100%|██████████| 18/18 [06:13<00:00, 20.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/3d-machine-learning/scannet/scene0708_00/train/rgb/890.jpg: Min Depth: 0.9114779233932495, Max Depth: 2.5443437099456787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}