{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align resulting depth map of Marigold with the COLMAP points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = './../../scannetpp/7eac902fd5'\n",
    "TRAIN_COLOR_DIR = os.path.join(DATA_DIR, 'train/rgb')\n",
    "TRAIN_DEPTH_DIR = os.path.join(DATA_DIR, 'train/depth')\n",
    "MG_TRAIN_DIR = os.path.join(DATA_DIR, 'train/depth_MG')\n",
    "MG_TRAIN_DIR_OUT = os.path.join(DATA_DIR, 'train/depth_MG_aligned')\n",
    "MG_TRAIN_UNCERTAINTY_DIR = os.path.join(DATA_DIR, 'train/uncertainty_MG')\n",
    "MG_TRAIN_UNCERTAINTY_DIR_OUT = os.path.join(DATA_DIR, 'train/uncertainty_MG_aligned')\n",
    "MG_TEST_DIR = os.path.join(DATA_DIR, 'test/depth_MG')\n",
    "COLORED_DEPTH_OUT = os.path.join(DATA_DIR, 'train/depth_MG_aligned_colored')\n",
    "COLMAP_DIR = os.path.join(DATA_DIR, 'colmap')\n",
    "COLMAP_TRAIN_DIR = os.path.join(COLMAP_DIR, 'sparse_train/0')\n",
    "COLMAP_ALL_DIR = os.path.join(COLMAP_DIR, 'sparse/0')\n",
    "POINT_CLOUDS_TRAIN_OUT = os.path.join(DATA_DIR, 'train/point_clouds_MG_aligned')\n",
    "BARBARA_POINT_CLOUDS_TRAIN_OUT = os.path.join(DATA_DIR, 'train/barbara_point_clouds')\n",
    "COLMAP_POINTS_TRAIN_OUT = os.path.join(DATA_DIR, 'train/colmap_point_cloud')\n",
    "MASK_PATH = os.path.join(DATA_DIR, 'eval_mask.pth')\n",
    "SENSOR_DEPTH_DIR = os.path.join(DATA_DIR, 'train/target_depth')\n",
    "SENSOR_DEPTH_SCALED_DIR = os.path.join(DATA_DIR, 'train/depth')\n",
    "SENSOR_POINT_CLOUD_OUT = os.path.join(DATA_DIR, 'train/sensor_point_cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if needed\n",
    "if not os.path.exists(MG_TRAIN_DIR_OUT):\n",
    "    os.makedirs(MG_TRAIN_DIR_OUT)\n",
    "if not os.path.exists(MG_TRAIN_UNCERTAINTY_DIR_OUT):\n",
    "    os.makedirs(MG_TRAIN_UNCERTAINTY_DIR_OUT)\n",
    "if not os.path.exists(COLORED_DEPTH_OUT):\n",
    "    os.makedirs(COLORED_DEPTH_OUT)\n",
    "if not os.path.exists(POINT_CLOUDS_TRAIN_OUT):\n",
    "    os.makedirs(POINT_CLOUDS_TRAIN_OUT)\n",
    "if not os.path.exists(BARBARA_POINT_CLOUDS_TRAIN_OUT):\n",
    "    os.makedirs(BARBARA_POINT_CLOUDS_TRAIN_OUT)\n",
    "if not os.path.exists(COLMAP_POINTS_TRAIN_OUT):\n",
    "    os.makedirs(COLMAP_POINTS_TRAIN_OUT)\n",
    "if not os.path.exists(SENSOR_POINT_CLOUD_OUT):\n",
    "    os.makedirs(SENSOR_POINT_CLOUD_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colmap_camera_matrix(colmap_cameras_path):\n",
    "    with open(colmap_cameras_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines if not line.startswith('#')]    # Skip all lines that begin with # (comments)\n",
    "        # Extract image size\n",
    "        image_size = tuple(map(int, lines[0].split(' ')[2:4]))\n",
    "        # Extract the camera parameters\n",
    "        camera_params = lines[0].split(' ')[-3:]\n",
    "        # Extract the focal length and principal point\n",
    "        f, cx, cy = map(float, camera_params)\n",
    "        # Construct the camera matrix\n",
    "        K = np.array([[f, 0, cx], [0, f, cy], [0, 0, 1]])\n",
    "    return image_size, K\n",
    "\n",
    "def get_colmap_dict_points(colmap_points_path):\n",
    "    colmap_points_dict = {}\n",
    "    with open(colmap_points_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines if not line.startswith('#')]    # Skip all lines that begin with # (comments)\n",
    "        for line in lines:\n",
    "            line = line.strip().split()\n",
    "            # 3D point list with one line of data per point:\n",
    "            # POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)\n",
    "            point_id = int(line[0])\n",
    "            x = float(line[1])\n",
    "            y = float(line[2])\n",
    "            z = float(line[3])\n",
    "            r = int(line[4])\n",
    "            g = int(line[5])\n",
    "            b = int(line[6])\n",
    "            error = float(line[7])\n",
    "            track = line[8:]\n",
    "            for i in range(8, len(line), 2):\n",
    "                image_id = int(line[i])\n",
    "                point2d_idx = int(line[i + 1])\n",
    "                track.append((image_id, point2d_idx))\n",
    "            colmap_points_dict[point_id] = (x, y, z, r, g, b, error, track)\n",
    "    return colmap_points_dict\n",
    "\n",
    "def get_colmap_points(colmap_images_path, image_id):\n",
    "    # Read the images.txt file which has the following format:\n",
    "    #   IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n",
    "    #   POINTS2D[] as (X, Y, POINT3D_ID)\n",
    "    with open(colmap_images_path) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines if not line.startswith('#')]    # Skip all lines that begin with # (comments)\n",
    "        # Find the line corresponding to the image_id\n",
    "        points_2d = []\n",
    "        points_3d_ids = []\n",
    "        q = None\n",
    "        t = None\n",
    "        for idx, line in enumerate(lines):\n",
    "            if str(line).endswith(str(image_id) + '.JPG'):\n",
    "                # Read the camera transformation\n",
    "                q = list(map(float, lines[idx].split(' ')[1:5]))\n",
    "                t = list(map(float, lines[idx].split(' ')[5:8]))\n",
    "                # lines[idx+1] contains the POINTS2D[] line\n",
    "                line_points = lines[idx+1].split(' ')\n",
    "                # Read three elements at a time\n",
    "                for i in range(0, len(line_points), 3):\n",
    "                    x, y, point_3d_id = line_points[i:i+3]\n",
    "                    if point_3d_id != '-1':\n",
    "                        points_2d.append((float(x), float(y)))   # Return (y, x) to fit the image coordinates in numpy\n",
    "                        points_3d_ids.append(int(point_3d_id))\n",
    "\n",
    "    return q, t, points_2d, points_3d_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def quaternion_to_rotation_matrix(q):\n",
    "    \"\"\"\n",
    "    Convert a quaternion into a 3x3 rotation matrix.\n",
    "    \"\"\"\n",
    "    qw, qx, qy, qz = q\n",
    "    R = np.array([\n",
    "        [1 - 2*qy**2 - 2*qz**2, 2*qx*qy - 2*qz*qw, 2*qx*qz + 2*qy*qw],\n",
    "        [2*qx*qy + 2*qz*qw, 1 - 2*qx**2 - 2*qz**2, 2*qy*qz - 2*qx*qw],\n",
    "        [2*qx*qz - 2*qy*qw, 2*qy*qz + 2*qx*qw, 1 - 2*qx**2 - 2*qy**2]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "def rotation_traslation_to_matrix(R, t):\n",
    "    \"\"\"\n",
    "    Convert a rotation matrix and a translation vector into a 4x4 transformation matrix.\n",
    "    \"\"\"\n",
    "    Tr = np.eye(4)\n",
    "    Tr[:3, :3] = R\n",
    "    Tr[:3, 3] = t\n",
    "    return Tr\n",
    "\n",
    "def transform_points(points_3d, Tr):\n",
    "    \"\"\"\n",
    "    Transform 3D points from world coordinates to camera coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    points_3d : ndarray of shape (n, 3)\n",
    "        The 3D points in world coordinates.\n",
    "    T : ndarray of shape (4, 4)\n",
    "        The transformation matrix from world to camera coordinates.\n",
    "    Returns:\n",
    "    points_3d_transformed : ndarray of shape (n, 3)\n",
    "        The transformed 3D points in camera coordinates.\n",
    "    \"\"\"\n",
    "    # Add a column of ones to the points_3d\n",
    "    points_3d_homogeneous = np.hstack([points_3d, np.ones((points_3d.shape[0], 1))])\n",
    "    # Transform the points\n",
    "    points_3d_transformed = (Tr @ points_3d_homogeneous.T).T[:, :3]\n",
    "    return points_3d_transformed\n",
    "\n",
    "def compute_depth(points_3d_transformed):\n",
    "    \"\"\"\n",
    "    Compute the depth values from the transformed 3D points.\n",
    "    \n",
    "    Parameters:\n",
    "    points_3d_transformed : ndarray of shape (n, 3)\n",
    "        The transformed 3D points in camera coordinates.\n",
    "    \n",
    "    Returns:\n",
    "    depths : ndarray of shape (n,)\n",
    "        The depth values (z-coordinates) of the transformed points.\n",
    "    \"\"\"\n",
    "    return points_3d_transformed[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_points(K, points_3d):\n",
    "    \"\"\"\n",
    "    Project 3D points onto the image plane using the intrinsic matrix K.\n",
    "    \n",
    "    Parameters:\n",
    "    K : ndarray of shape (3, 3)\n",
    "        The intrinsic matrix.\n",
    "    points_3d : ndarray of shape (n, 3)\n",
    "        The 3D points in camera coordinates.\n",
    "    \n",
    "    Returns:\n",
    "    points_2d : ndarray of shape (n, 2)\n",
    "        The projected 2D points on the image plane.\n",
    "    depths : ndarray of shape (n,)\n",
    "        The depth values (z-coordinates) of the transformed points.\n",
    "    \"\"\"\n",
    "    # Convert 3D points to homogeneous coordinates\n",
    "    # points_3d_h = np.hstack([points_3d, np.ones((points_3d.shape[0], 1))])\n",
    "    \n",
    "    # Project to 2D (homogeneous coordinates)\n",
    "    points_2d_h = (K @ points_3d.T).T\n",
    "    \n",
    "    # Convert to non-homogeneous coordinates\n",
    "    points_2d = points_2d_h[:, :2] / points_2d_h[:, 2:]\n",
    "    depths = points_3d[:, 2]  # Extract depth values (z-coordinates)\n",
    "    \n",
    "    return points_2d, depths\n",
    "\n",
    "def filter_points_in_image(points_2d, depths, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Filter points to determine which ones lie within the camera plane.\n",
    "    \n",
    "    Parameters:\n",
    "    points_2d : ndarray of shape (n, 2)\n",
    "        The projected 2D points on the image plane.\n",
    "    depths : ndarray of shape (n,)\n",
    "        The depth values of the points.\n",
    "    image_width : int\n",
    "        The width of the image.\n",
    "    image_height : int\n",
    "        The height of the image.\n",
    "    \n",
    "    Returns:\n",
    "    mask : ndarray of shape (n,)\n",
    "        A boolean array indicating which points are within the image boundaries and in front of the camera.\n",
    "    \"\"\"\n",
    "    mask = (\n",
    "        (points_2d[:, 0] >= 0) & (points_2d[:, 0] < image_width) &  # x coordinates within image width\n",
    "        (points_2d[:, 1] >= 0) & (points_2d[:, 1] < image_height) &  # y coordinates within image height\n",
    "        (depths > 0)  # points in front of the camera\n",
    "    )\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_depth(depth_map, points_2d):\n",
    "    # Define the grid coordinates\n",
    "    y = np.arange(depth_map.shape[1])\n",
    "    x = np.arange(depth_map.shape[0])\n",
    "\n",
    "    # Create the interpolator object (we transpose because points_2d is (x, y) while depth_map is (y, x))\n",
    "    interpolator = RegularGridInterpolator((x, y), depth_map)\n",
    "\n",
    "    # Interpolate the depth values at the 2D points\n",
    "    interpolated_depths = interpolator(points_2d)\n",
    "\n",
    "    return interpolated_depths\n",
    "\n",
    "def align_depth_map_with_3d_points(depth_map, points_2d, ground_truth_depths, points_error):\n",
    "    # Interpolate the depth values at the 2D points\n",
    "    depths = interpolate_depth(depth_map, points_2d)\n",
    "    # print(f'MG_PREDICTION: {depths}')\n",
    "    # print(f'COLMAP_GT: {ground_truth_depths}')\n",
    "    print(f'Error before: {np.sqrt(np.mean((depths - ground_truth_depths)**2))}')\n",
    "\n",
    "    # Reject outliers\n",
    "    # inds1 = np.argsort(depths)\n",
    "    # inds2 = np.argsort(ground_truth_depths)\n",
    "\n",
    "    # mask = (np.abs(inds1 - inds2) < (depths.shape[0] // 4))  & (depths >= 0.1) & (depths <= 0.9)\n",
    "    # ground_truth_depths = ground_truth_depths[mask]\n",
    "    # points_error = points_error[mask]\n",
    "    # depths = depths[mask]\n",
    "\n",
    "    # Calculate the square root of the weights (use inverse error as weights and consider the relative error per point)\n",
    "    #sqrt_weights = np.sqrt(1.0 / ((points_error ** 2))) # Note using the squared error (error ** 4 here) gives worse results\n",
    "    if points_error == None:\n",
    "        sqrt_weights = np.sqrt(1.0 / (ground_truth_depths ** 2))\n",
    "    else:\n",
    "        sqrt_weights = np.sqrt(1.0 / ((ground_truth_depths ** 2) * (points_error ** 4)))\n",
    "    \n",
    "    # Set up the weighted linear system: sqrt(W) * (A @ [a, b]) = sqrt(W) * depth_ground_truth_valid\n",
    "    A = torch.tensor(np.vstack([depths, np.ones_like(depths)]).T, dtype=torch.float32)\n",
    "    b = torch.tensor(ground_truth_depths, dtype=torch.float32)\n",
    "    W = torch.tensor(np.diag(sqrt_weights), dtype=torch.float32)\n",
    "\n",
    "    # Solve for the scale (a) and shift (b) parameters using the weighted system\n",
    "    params = torch.linalg.inv(A.T @ W @ A) @ A.T @ W @ b\n",
    "    \n",
    "    # Extract scale and shift\n",
    "    s, t = params\n",
    "    s = s.numpy()\n",
    "    t = t.numpy()\n",
    "\n",
    "    # Compute the error\n",
    "    error = np.sqrt(np.mean((s * depths + t - ground_truth_depths)**2))\n",
    "    print(f'Error after: {error}')\n",
    "    print(f'Scale : {s}, Shift: {t}')\n",
    "    \n",
    "    return s, t, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_depth_maps(depth_map, min_depth, max_depth, cmap=\"Spectral\"):\n",
    "    \"\"\"\n",
    "    Colorize depth maps.\n",
    "    \"\"\"\n",
    "    assert len(depth_map.shape) >= 2, \"Invalid dimension\"\n",
    "\n",
    "    depth = depth_map.copy().squeeze()\n",
    "    # reshape to [ (B,) H, W ]\n",
    "    if depth.ndim < 3:\n",
    "        depth = depth[np.newaxis, :, :]\n",
    "\n",
    "    # colorize\n",
    "    cm = matplotlib.colormaps[cmap]\n",
    "    depth = ((depth - min_depth) / (max_depth - min_depth)).clip(0, 1)\n",
    "    img_colored_np = cm(depth, bytes=False)[:, :, :, 0:3]  # value from 0 to 1\n",
    "    img_colored_np = np.rollaxis(img_colored_np, 3, 1)\n",
    "\n",
    "    return img_colored_np\n",
    "\n",
    "def chw2hwc(chw):\n",
    "    assert 3 == len(chw.shape)\n",
    "    if isinstance(chw, np.ndarray):\n",
    "        hwc = np.moveaxis(chw, 0, -1)\n",
    "    return hwc\n",
    "\n",
    "def get_colorized_depth_map(depth_pred, color_map=\"Spectral\", valid_mask=None):\n",
    "    min_depth = 0\n",
    "    max_depth = 20\n",
    "    depth_colored = colorize_depth_maps(depth_pred, min_depth, max_depth, cmap=color_map).squeeze()  # [3, H, W], value in (0, 1)\n",
    "    depth_colored = (depth_colored * 255).astype(np.uint8)\n",
    "    depth_colored_hwc = chw2hwc(depth_colored)\n",
    "    depth_colored_img = Image.fromarray(depth_colored_hwc)\n",
    "    return depth_colored_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save point cloud to obj file\n",
    "def save_point_cloud_to_obj(points, filename, colors=None):\n",
    "    with open(filename  + '.obj', 'w') as f:\n",
    "        for i in range(points.shape[0]):\n",
    "            # Write the vertices with colors\n",
    "            point = points[i, :]\n",
    "            if colors is not None:\n",
    "                color = colors[i, :]\n",
    "                f.write('v %f %f %f %f %f %f\\n' % (point[0], point[1], point[2], color[0], color[1], color[2]))\n",
    "            else:\n",
    "                f.write('v %f %f %f\\n' % (point[0], point[1], point[2]))\n",
    "\n",
    "def save_point_cloud(metric_depth_pred, image, K, camera_to_world, out_path, mask=None):\n",
    "    # Prepare colors array\n",
    "    colors = image.reshape(-1, 3)\n",
    "\n",
    "    # Create grid for the image plane\n",
    "    FINAL_WIDTH, FINAL_HEIGHT = image.shape[1], image.shape[0]\n",
    "    x, y = np.meshgrid(np.arange(FINAL_WIDTH), np.arange(FINAL_HEIGHT))\n",
    "\n",
    "    # Normalize coordinates using the intrinsic matrix parameters\n",
    "    focal_length_x = K[0, 0]\n",
    "    focal_length_y = K[1, 1]\n",
    "    c_x = K[0, 2]\n",
    "    c_y = K[1, 2]\n",
    "    x_normalized = (x - c_x) / focal_length_x\n",
    "    y_normalized = (FINAL_HEIGHT - y - c_y) / focal_length_y\n",
    "\n",
    "    # Depth values from resized_pred\n",
    "    z = np.array(metric_depth_pred)\n",
    "    # Calculate 3D points by unprojecting\n",
    "    points = np.stack((np.multiply(x_normalized, z), np.multiply(y_normalized, z), -z), axis=-1).reshape(-1, 3)\n",
    "    # Convert points to homogeneous coordinates\n",
    "    points = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    # Convert points from camera coordinates to world coordinates\n",
    "    points = camera_to_world.dot(points.T).T\n",
    "    # Get (x, y, z) coordinates\n",
    "    points = points[:, :3]\n",
    "    if mask is not None:\n",
    "        save_point_cloud_to_obj(points[mask], out_path, colors[mask])\n",
    "    else:\n",
    "        save_point_cloud_to_obj(points, out_path, colors)\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_depth_completion_scaling_to_m(depth):\n",
    "    # convert from depth completion scaling to meter, that means map range 0 .. 1 to range 0 .. 16,38m\n",
    "    return depth * (2 ** 16 - 1) / 4000.\n",
    "\n",
    "def compute_rmse(prediction, target):\n",
    "    return torch.sqrt((prediction - target).pow(2).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "import json\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "meta = load_json(os.path.join(DATA_DIR, 'transforms_train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE DSC01534:\n",
      "NO POINTS FOR IMAGE DSC01534\n",
      "IMAGE DSC01566:\n",
      "Num points: (223,)\n",
      "Max barbaras depth: 1.688\n",
      "980\n",
      "1724\n",
      "Error before: 0.47861374139304874\n",
      "Error after: 0.0632730332773651\n",
      "Scale : 1.1044807434082031, Shift: 0.42906099557876587\n",
      "MAX DEPTH: 1.5335416793823242, MAX DEPTH GT: 1.688\n",
      "MIN DEPTH: 0.42906099557876587, MIN DEPTH GT: 0.459\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01566.png' will be overwritten\n",
      "Max aligned: 1.5335417\n",
      "Max sensor: 1.943\n",
      "\n",
      "\n",
      "IMAGE DSC01550:\n",
      "Num points: (449,)\n",
      "Max barbaras depth: 1.9\n",
      "1143\n",
      "1729\n",
      "Error before: 0.6074934316421875\n",
      "Error after: 0.08352698085214355\n",
      "Scale : 0.9258283376693726, Shift: 0.6225485801696777\n",
      "MAX DEPTH: 1.5483769178390503, MAX DEPTH GT: 1.9\n",
      "MIN DEPTH: 0.6225485801696777, MIN DEPTH GT: 0.522\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01550.png' will be overwritten\n",
      "Max aligned: 1.5483769\n",
      "Max sensor: 1.968\n",
      "\n",
      "\n",
      "IMAGE DSC01542:\n",
      "Num points: (979,)\n",
      "Max barbaras depth: 2.009\n",
      "1131\n",
      "1717\n",
      "Error before: 0.7817974611606191\n",
      "Error after: 0.11322411550418432\n",
      "Scale : 1.3253841400146484, Shift: 0.633398711681366\n",
      "MAX DEPTH: 1.9587829113006592, MAX DEPTH GT: 2.009\n",
      "MIN DEPTH: 0.633398711681366, MIN DEPTH GT: 0.537\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01542.png' will be overwritten\n",
      "Max aligned: 1.9587829\n",
      "Max sensor: 2.107\n",
      "\n",
      "\n",
      "IMAGE DSC01558:\n",
      "Num points: (90,)\n",
      "Max barbaras depth: 0.801\n",
      "876\n",
      "1682\n",
      "Error before: 0.14625908515931735\n",
      "Error after: 0.06906278576885655\n",
      "Scale : 0.24493408203125, Shift: 0.5497413277626038\n",
      "MAX DEPTH: 0.7946754097938538, MAX DEPTH GT: 0.801\n",
      "MIN DEPTH: 0.5497413277626038, MIN DEPTH GT: 0.574\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01558.png' will be overwritten\n",
      "Max aligned: 0.7946754\n",
      "Max sensor: 1.297\n",
      "\n",
      "\n",
      "IMAGE DSC01350:\n",
      "Num points: (360,)\n",
      "Max barbaras depth: 2.442\n",
      "1076\n",
      "1103\n",
      "Error before: 1.266487118243203\n",
      "Error after: 0.587770272822909\n",
      "Scale : -1.1170198917388916, Shift: 2.2874982357025146\n",
      "MAX DEPTH: 2.2874982357025146, MAX DEPTH GT: 2.442\n",
      "MIN DEPTH: 1.170478343963623, MIN DEPTH GT: 0.629\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01350.png' will be overwritten\n",
      "Max aligned: 2.2874982\n",
      "Max sensor: 1.967\n",
      "\n",
      "\n",
      "IMAGE DSC01526:\n",
      "Num points: (503,)\n",
      "Max barbaras depth: 3.204\n",
      "1097\n",
      "1502\n",
      "Error before: 1.5730794413519917\n",
      "Error after: 0.7988000756155474\n",
      "Scale : 0.5591899156570435, Shift: 1.3214266300201416\n",
      "MAX DEPTH: 1.880616545677185, MAX DEPTH GT: 3.204\n",
      "MIN DEPTH: 1.3214266300201416, MIN DEPTH GT: 0.679\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01526.png' will be overwritten\n",
      "Max aligned: 1.8806165\n",
      "Max sensor: 2.248\n",
      "\n",
      "\n",
      "IMAGE DSC01358:\n",
      "Num points: (305,)\n",
      "Max barbaras depth: 3.067\n",
      "1138\n",
      "1408\n",
      "Error before: 1.4343315801028496\n",
      "Error after: 0.8131159705023495\n",
      "Scale : 2.6240761280059814, Shift: -0.4755000174045563\n",
      "MAX DEPTH: 2.148564577102661, MAX DEPTH GT: 3.067\n",
      "MIN DEPTH: -0.4755000174045563, MIN DEPTH GT: 0.358\n",
      "NEGATIVE DEPTH HERE: DSC01358.npy, ID: 7\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01358.png' will be overwritten\n",
      "Max aligned: 2.1485646\n",
      "Max sensor: 1.303\n",
      "\n",
      "\n",
      "IMAGE DSC01366:\n",
      "Num points: (568,)\n",
      "Max barbaras depth: 2.586\n",
      "1141\n",
      "1719\n",
      "Error before: 1.204504727209475\n",
      "Error after: 0.5136798603324514\n",
      "Scale : 1.376773715019226, Shift: 0.7968482971191406\n",
      "MAX DEPTH: 2.173603057861328, MAX DEPTH GT: 2.586\n",
      "MIN DEPTH: 0.7968482971191406, MIN DEPTH GT: 0.552\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01366.png' will be overwritten\n",
      "Max aligned: 2.173603\n",
      "Max sensor: 2.377\n",
      "\n",
      "\n",
      "IMAGE DSC01510:\n",
      "Num points: (155,)\n",
      "Max barbaras depth: 1.673\n",
      "1129\n",
      "1706\n",
      "Error before: 0.41284945649738286\n",
      "Error after: 0.06259798606249808\n",
      "Scale : 1.0378167629241943, Shift: 0.37948939204216003\n",
      "MAX DEPTH: 1.4173061847686768, MAX DEPTH GT: 1.673\n",
      "MIN DEPTH: 0.37948939204216003, MIN DEPTH GT: 0.586\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01510.png' will be overwritten\n",
      "Max aligned: 1.4173062\n",
      "Max sensor: 1.798\n",
      "\n",
      "\n",
      "IMAGE DSC01502:\n",
      "Num points: (219,)\n",
      "Max barbaras depth: 1.507\n",
      "1129\n",
      "1325\n",
      "Error before: 0.4181337366934414\n",
      "Error after: 0.05881326708557315\n",
      "Scale : 0.9138418436050415, Shift: 0.4738927483558655\n",
      "MAX DEPTH: 1.3877346515655518, MAX DEPTH GT: 1.507\n",
      "MIN DEPTH: 0.4738927483558655, MIN DEPTH GT: 0.714\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01502.png' will be overwritten\n",
      "Max aligned: 1.3877347\n",
      "Max sensor: 1.602\n",
      "\n",
      "\n",
      "IMAGE DSC01342:\n",
      "Num points: (91,)\n",
      "Max barbaras depth: 1.567\n",
      "1088\n",
      "1698\n",
      "Error before: 0.39033132659091924\n",
      "Error after: 0.09544709856518281\n",
      "Scale : 1.029127836227417, Shift: 0.35323795676231384\n",
      "MAX DEPTH: 1.382361888885498, MAX DEPTH GT: 1.567\n",
      "MIN DEPTH: 0.35323795676231384, MIN DEPTH GT: 0.456\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01342.png' will be overwritten\n",
      "Max aligned: 1.3823619\n",
      "Max sensor: 1.546\n",
      "\n",
      "\n",
      "IMAGE DSC01326:\n",
      "Num points: (193,)\n",
      "Max barbaras depth: 2.059\n",
      "961\n",
      "1521\n",
      "Error before: 0.5508266835299884\n",
      "Error after: 0.09292784031144337\n",
      "Scale : 1.5284602642059326, Shift: 0.21526339650154114\n",
      "MAX DEPTH: 1.7437236309051514, MAX DEPTH GT: 2.059\n",
      "MIN DEPTH: 0.21526339650154114, MIN DEPTH GT: 0.587\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01326.png' will be overwritten\n",
      "Max aligned: 1.7437236\n",
      "Max sensor: 2.036\n",
      "\n",
      "\n",
      "IMAGE DSC01286:\n",
      "Num points: (691,)\n",
      "Max barbaras depth: 3.366\n",
      "1134\n",
      "1713\n",
      "Error before: 0.7133300940812918\n",
      "Error after: 0.1036067027627102\n",
      "Scale : 1.6867163181304932, Shift: 0.39499932527542114\n",
      "MAX DEPTH: 2.0817155838012695, MAX DEPTH GT: 3.366\n",
      "MIN DEPTH: 0.39499932527542114, MIN DEPTH GT: 0.453\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01286.png' will be overwritten\n",
      "Max aligned: 2.0817156\n",
      "Max sensor: 2.282\n",
      "\n",
      "\n",
      "IMAGE DSC01278:\n",
      "Num points: (1518,)\n",
      "Max barbaras depth: 2.517\n",
      "1104\n",
      "1722\n",
      "Error before: 0.5193223265832083\n",
      "Error after: 0.05554730342406186\n",
      "Scale : 1.854722261428833, Shift: 0.31948649883270264\n",
      "MAX DEPTH: 2.174208641052246, MAX DEPTH GT: 2.517\n",
      "MIN DEPTH: 0.31948649883270264, MIN DEPTH GT: 0.336\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01278.png' will be overwritten\n",
      "Max aligned: 2.1742086\n",
      "Max sensor: 2.491\n",
      "\n",
      "\n",
      "IMAGE DSC01318:\n",
      "Num points: (274,)\n",
      "Max barbaras depth: 2.501\n",
      "1142\n",
      "1676\n",
      "Error before: 0.47853286133534706\n",
      "Error after: 0.1251156958841491\n",
      "Scale : 1.077358603477478, Shift: 0.39488664269447327\n",
      "MAX DEPTH: 1.472245216369629, MAX DEPTH GT: 2.501\n",
      "MIN DEPTH: 0.39488664269447327, MIN DEPTH GT: 0.562\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01318.png' will be overwritten\n",
      "Max aligned: 1.4722452\n",
      "Max sensor: 1.695\n",
      "\n",
      "\n",
      "IMAGE DSC01294:\n",
      "Num points: (194,)\n",
      "Max barbaras depth: 2.555\n",
      "1120\n",
      "1434\n",
      "Error before: 0.8347469963341598\n",
      "Error after: 0.12228604391265413\n",
      "Scale : 1.4186619520187378, Shift: 0.45826730132102966\n",
      "MAX DEPTH: 1.8769292831420898, MAX DEPTH GT: 2.555\n",
      "MIN DEPTH: 0.45826730132102966, MIN DEPTH GT: 0.698\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01294.png' will be overwritten\n",
      "Max aligned: 1.8769293\n",
      "Max sensor: 2.101\n",
      "\n",
      "\n",
      "IMAGE DSC01382:\n",
      "Num points: (899,)\n",
      "Max barbaras depth: 2.454\n",
      "1101\n",
      "1610\n",
      "Error before: 0.7150893714180032\n",
      "Error after: 0.12674546098997597\n",
      "Scale : 1.5533947944641113, Shift: 0.39574134349823\n",
      "MAX DEPTH: 1.9491361379623413, MAX DEPTH GT: 2.454\n",
      "MIN DEPTH: 0.39574134349823, MIN DEPTH GT: 0.595\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01382.png' will be overwritten\n",
      "Max aligned: 1.9491361\n",
      "Max sensor: 2.486\n",
      "\n",
      "\n",
      "IMAGE DSC01406:\n",
      "Num points: (1035,)\n",
      "Max barbaras depth: 2.126\n",
      "1109\n",
      "1684\n",
      "Error before: 0.4926092454229781\n",
      "Error after: 0.060164556426000215\n",
      "Scale : 1.4853851795196533, Shift: 0.28893551230430603\n",
      "MAX DEPTH: 1.7743102312088013, MAX DEPTH GT: 2.126\n",
      "MIN DEPTH: 0.28893551230430603, MIN DEPTH GT: 0.497\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01406.png' will be overwritten\n",
      "Max aligned: 1.7743102\n",
      "Max sensor: 2.173\n",
      "\n",
      "\n",
      "IMAGE DSC01374:\n",
      "Num points: (977,)\n",
      "Max barbaras depth: 2.189\n",
      "1120\n",
      "1710\n",
      "Error before: 0.8196922665395702\n",
      "Error after: 0.11148617686881322\n",
      "Scale : 1.4465144872665405, Shift: 0.5952858328819275\n",
      "MAX DEPTH: 2.0418002605438232, MAX DEPTH GT: 2.189\n",
      "MIN DEPTH: 0.5952858328819275, MIN DEPTH GT: 0.46\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01374.png' will be overwritten\n",
      "Max aligned: 2.0418003\n",
      "Max sensor: 2.196\n",
      "\n",
      "\n",
      "IMAGE DSC01414:\n",
      "Num points: (440,)\n",
      "Max barbaras depth: 1.46\n",
      "1131\n",
      "1602\n",
      "Error before: 0.47815483099322575\n",
      "Error after: 0.026537156094677015\n",
      "Scale : 0.8339855670928955, Shift: 0.553665280342102\n",
      "MAX DEPTH: 1.3876508474349976, MAX DEPTH GT: 1.46\n",
      "MIN DEPTH: 0.553665280342102, MIN DEPTH GT: 0.483\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01414.png' will be overwritten\n",
      "Max aligned: 1.3876508\n",
      "Max sensor: 1.425\n",
      "\n",
      "\n",
      "IMAGE DSC01462:\n",
      "Num points: (1892,)\n",
      "Max barbaras depth: 1.892\n",
      "949\n",
      "1352\n",
      "Error before: 0.5760904520503367\n",
      "Error after: 0.05288371377993203\n",
      "Scale : 0.8662074208259583, Shift: 0.611286997795105\n",
      "MAX DEPTH: 1.477494478225708, MAX DEPTH GT: 1.892\n",
      "MIN DEPTH: 0.611286997795105, MIN DEPTH GT: 0.719\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01462.png' will be overwritten\n",
      "Max aligned: 1.4774945\n",
      "Max sensor: 1.998\n",
      "\n",
      "\n",
      "IMAGE DSC01438:\n",
      "Num points: (549,)\n",
      "Max barbaras depth: 1.104\n",
      "876\n",
      "780\n",
      "Error before: 0.3796232741755687\n",
      "Error after: 0.026823450509746133\n",
      "Scale : 0.8632381558418274, Shift: 0.40936964750289917\n",
      "MAX DEPTH: 1.2726078033447266, MAX DEPTH GT: 1.104\n",
      "MIN DEPTH: 0.40936964750289917, MIN DEPTH GT: 0.469\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01438.png' will be overwritten\n",
      "Max aligned: 1.2726078\n",
      "Max sensor: 1.741\n",
      "\n",
      "\n",
      "IMAGE DSC01470:\n",
      "Num points: (1082,)\n",
      "Max barbaras depth: 1.302\n",
      "1140\n",
      "1297\n",
      "Error before: 0.5133701199719701\n",
      "Error after: 0.07543691353035109\n",
      "Scale : 0.9797017574310303, Shift: 0.5081674456596375\n",
      "MAX DEPTH: 1.4878692626953125, MAX DEPTH GT: 1.302\n",
      "MIN DEPTH: 0.5081674456596375, MIN DEPTH GT: 0.514\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01470.png' will be overwritten\n",
      "Max aligned: 1.4878693\n",
      "Max sensor: 1.539\n",
      "\n",
      "\n",
      "IMAGE DSC01478:\n",
      "Num points: (1557,)\n",
      "Max barbaras depth: 2.378\n",
      "1139\n",
      "1728\n",
      "Error before: 0.7068973668427688\n",
      "Error after: 0.08494898150769821\n",
      "Scale : 1.4307117462158203, Shift: 0.5887993574142456\n",
      "MAX DEPTH: 2.0195112228393555, MAX DEPTH GT: 2.378\n",
      "MIN DEPTH: 0.5887993574142456, MIN DEPTH GT: 0.51\n",
      "Existing file: './../../scannetpp/7eac902fd5/train/depth_MG_aligned_colored/DSC01478.png' will be overwritten\n",
      "Max aligned: 2.0195112\n",
      "Max sensor: 2.344\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "barbara_depths = torch.load(os.path.join(DATA_DIR, 'depths.pth'))\n",
    "# Get camera matrix\n",
    "image_size, K = get_colmap_camera_matrix(os.path.join(COLMAP_TRAIN_DIR, 'cameras.txt'))\n",
    "image_width, image_height = image_size\n",
    "# Get dictionary of points\n",
    "points_dict = get_colmap_dict_points(os.path.join(COLMAP_TRAIN_DIR, 'points3D.txt'))\n",
    "errors = []\n",
    "rmse_erros = []\n",
    "aligned_depths = []\n",
    "sensor_depths = []\n",
    "valid_indexes = []\n",
    "\n",
    "# Iterate over the depths in the directory\n",
    "for i, meta_i in enumerate(meta['frames']): #os.listdir(MG_TRAIN_DIR)):\n",
    "    filename = meta_i['file_path']\n",
    "    filename = os.path.basename(filename).split('.')[0] + '.npy'\n",
    "    if not filename.endswith('uncertainty.npy'):\n",
    "        image_id = filename.split('.')[0]\n",
    "        K[0, 0] = meta_i['fx']\n",
    "        K[1, 1] = meta_i['fy']\n",
    "        K[0, 2] = meta_i['cx']\n",
    "        K[1, 2] = meta_i['cy']\n",
    "        print(f'IMAGE {image_id}:')\n",
    "        # Get relative depth from Marigold\n",
    "        relative_depth = np.load(os.path.join(MG_TRAIN_DIR, filename))\n",
    "        image = np.array(Image.open(os.path.join(DATA_DIR, meta_i['file_path']))) / 255.0\n",
    "        # Invert the transformation matrix to get the camera-to-world matrix\n",
    "        camera_to_world = np.array(meta_i['transform_matrix'])\n",
    "        # Get points from COLMAP for the same image\n",
    "        # LEGACY CODE FOR USING COLMAP POINTS\n",
    "        # colmap_images_path = os.path.join(COLMAP_TRAIN_DIR, 'images.txt')\n",
    "        # q, t, _, points_3d_ids = get_colmap_points(colmap_images_path, image_id)\n",
    "        # points_3d = [points_dict[point_id] for point_id in points_3d_ids]\n",
    "        # # Select the first three elements of the points_3d\n",
    "        # points_3d_xyz = np.array([point[:3] for point in points_3d])\n",
    "        # points_error = np.array([point[6] for point in points_3d])\n",
    "        # # Transform the 3D points into camera coordinates\n",
    "        # print(image_id)\n",
    "        # Tr = rotation_traslation_to_matrix(quaternion_to_rotation_matrix(q), t)\n",
    "        # if points_3d_xyz.shape[0] == 0:\n",
    "        #     print(f'NO POINTS FOR IMAGE {image_id}')\n",
    "        #     continue\n",
    "        # points_3d_transformed = transform_points(np.array(points_3d_xyz)[:, :3], Tr)\n",
    "        # # Sanity check. Project points and get depths\n",
    "        # points_2d, depths = project_points(K, points_3d_transformed)\n",
    "        # # Sanity check. Filter points within the image boundaries\n",
    "        # mask = filter_points_in_image(points_2d, depths, image_width, image_height)\n",
    "        # if mask.any() == False:\n",
    "        #     # This should never happen if the points from COLMAP are correct\n",
    "        #     print(f'WARNING, ALL COLMAP POINTS SHOULD LIE INSIDE THE IMAGE PLANE {image_id}')\n",
    "        #     continue\n",
    "        # END LEGACY CODE\n",
    "        # Recover metric depth from the colmap points\n",
    "        depth = np.array(Image.open(os.path.join(DATA_DIR, meta_i['depth_file_path'])), dtype=np.float64) / 1000.\n",
    "        # Get indexes for the pixels with actual depth\n",
    "        points_2d = np.argwhere(depth > 0)\n",
    "        if points_2d.shape[0] == 0:\n",
    "            print(f'NO POINTS FOR IMAGE {image_id}')\n",
    "            continue\n",
    "        valid_indexes.append(i)\n",
    "        # Get the depth values using indexes\n",
    "        depths = depth[points_2d[:, 0], points_2d[:, 1]]\n",
    "        print('Num points:', depths.shape)\n",
    "        print('Max COLMAP depth:', depths.max())\n",
    "        # print(points_2d[])\n",
    "        # Correct the points 2d to adapt them to a cropped image\n",
    "        points_2d[:, 0] = points_2d[:, 0] - 8  # Width correction\n",
    "        points_2d[:, 1] = points_2d[:, 1] - 6  # Height correction\n",
    "        mask = (points_2d[:, 0] >= 0 ) & (points_2d[:, 0] < depth.shape[0]) & (points_2d[:, 1] >= 0) & (points_2d[:, 1] < depth.shape[1])\n",
    "        points_2d = points_2d[mask]\n",
    "        depths = depths[mask]\n",
    "        # Align the depth map with the 3D points\n",
    "        s, t, error = align_depth_map_with_3d_points(relative_depth, points_2d, depths, None)  # TODO: check the points errors also\n",
    "        errors.append(error)\n",
    "        # Modify the depth map using the found scale and shift\n",
    "        aligned_depth = s * relative_depth + t\n",
    "        print(f'MAX DEPTH: {aligned_depth.max()}, MAX DEPTH GT: {depths.max()}')\n",
    "        print(f'MIN DEPTH: {aligned_depth.min()}, MIN DEPTH GT: {depths.min()}')\n",
    "        if aligned_depth.min() < 0:\n",
    "            print(f'NEGATIVE DEPTH HERE: {filename}, ID: {i}')\n",
    "        # Save the aligned depth map\n",
    "        np.save(os.path.join(MG_TRAIN_DIR_OUT, filename), aligned_depth)\n",
    "        # Scale the uncertainty values\n",
    "        uncertainty_filename = filename.replace('.npy', '_uncertainty.npy')\n",
    "        uncertainty = np.load(os.path.join(MG_TRAIN_UNCERTAINTY_DIR, uncertainty_filename))\n",
    "        np.save(os.path.join(MG_TRAIN_UNCERTAINTY_DIR_OUT, uncertainty_filename), uncertainty * s)\n",
    "        # Plot the depth map\n",
    "        colored_save_path = os.path.join(COLORED_DEPTH_OUT, filename.replace('.npy', '.png'))\n",
    "        if os.path.exists(colored_save_path):\n",
    "            print(f\"Existing file: '{colored_save_path}' will be overwritten\")\n",
    "        depth_colored = get_colorized_depth_map(aligned_depth)\n",
    "        depth_colored.save(colored_save_path)\n",
    "        # Unproject and save point cloud\n",
    "        save_point_cloud(aligned_depth, image, K, camera_to_world, os.path.join(POINT_CLOUDS_TRAIN_OUT, image_id))\n",
    "        print('Max aligned:', aligned_depth.max())\n",
    "        aligned_depths.append(aligned_depth)\n",
    "        # Visualize COLMAP points\n",
    "        # colors = np.zeros_like(points_3d_xyz)\n",
    "        # colors[:, 0] = 1.0\n",
    "        # save_point_cloud_to_obj(points_3d_xyz, os.path.join(COLMAP_POINTS_TRAIN_OUT, image_id), colors)\n",
    "        # Load sensor depth and save it as a pointcloud\n",
    "        sensor_depth = np.array(Image.open(os.path.join(SENSOR_DEPTH_DIR, filename.replace('.npy', '.png'))), dtype=np.float64) / 1000.0\n",
    "        sensor_depth_mask = sensor_depth.flatten() > 0\n",
    "        print('Max sensor:', sensor_depth.max())\n",
    "        image = image[6:-6, 8:-8, :]\n",
    "        save_point_cloud(sensor_depth, image, K, camera_to_world, os.path.join(SENSOR_POINT_CLOUD_OUT, image_id), sensor_depth_mask)\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth_depth(basedir, train_filenames, image_size, depth_scaling_factor):\n",
    "    H, W = image_size\n",
    "    gt_depths = []\n",
    "    gt_valid_depths = []\n",
    "    for filename in train_filenames:\n",
    "        filename = filename.replace(\"rgb\", \"target_depth\")\n",
    "        filename = filename.replace(\".jpg\", \".png\")\n",
    "        gt_depth_fname = os.path.join(basedir, filename)\n",
    "        if os.path.exists(gt_depth_fname):\n",
    "            gt_depth = np.array(Image.open(gt_depth_fname), dtype=np.float64)\n",
    "            gt_valid_depth = gt_depth > 0.5\n",
    "            gt_depth = (gt_depth / depth_scaling_factor).astype(np.float32)\n",
    "        else:\n",
    "            gt_depth = np.zeros((H, W))\n",
    "            gt_valid_depth = np.full_like(gt_depth, False)\n",
    "        gt_depths.append(np.expand_dims(gt_depth, -1))\n",
    "        gt_valid_depths.append(gt_valid_depth)\n",
    "    gt_depths = np.stack(gt_depths, 0)\n",
    "    gt_valid_depths = np.stack(gt_valid_depths, 0)\n",
    "    return gt_depths, gt_valid_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_imgs = torch.tensor(list(range(1, len(meta['frames']))))\n",
    "# eval_mask = torch.load(MASK_PATH)[valid_imgs, :, :]\n",
    "sensor_depth, _ = load_ground_truth_depth(DATA_DIR, [frame['file_path'] for frame in meta['frames']], (1156, 1736), 1000)\n",
    "sensor_depth = torch.tensor(sensor_depth, dtype=torch.float32)[valid_imgs, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 4688436\n",
    "# fig, axes = plt.subplots(1, 2)\n",
    "# axes[0].hist(torch.sort((x - y).square())[0][int(0.9 * n):], bins=100)\n",
    "# axes[1].hist(torch.sort((z - y).square())[0][int(0.9 * n):], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depths = torch.tensor(aligned_depths).clamp(min=near, max=far)\n",
    "# x, y = depths[:, :, :][eval_mask], gt_depths_train.squeeze(-1)[eval_mask]\n",
    "# z = torch.tensor(barbara_depths).clamp(min=near, max=far)[eval_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse:  tensor(1.1790)\n",
      "l1:  tensor(1.0876)\n"
     ]
    }
   ],
   "source": [
    "# Our errors\n",
    "near = 0.10000000149011612\n",
    "far = 6.8439249992370605\n",
    "depths = torch.tensor(aligned_depths)[:, 6:-6, 8:-8]   # Crop the depth image from Marigold to match the extracted depth shape\n",
    "# depths = torch.tensor(aligned_depths).clamp(min=near, max=far)\n",
    "gt_depths_train = sensor_depth #torch.tensor(sensor_depths, dtype=torch.float32) / 1000.0\n",
    "\n",
    "print('rmse: ', compute_rmse(depths, gt_depths_train.squeeze(-1)))\n",
    "print('l1: ', torch.abs(depths - gt_depths_train.squeeze(-1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'barbara_depths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m near \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.10000000149011612\u001b[39m\n\u001b[1;32m      3\u001b[0m far \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6.8439249992370605\u001b[39m\n\u001b[0;32m----> 4\u001b[0m b_depths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mbarbara_depths\u001b[49m[valid_imgs, :, :])\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39mnear, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mfar)\n\u001b[1;32m      5\u001b[0m gt_depths_train \u001b[38;5;241m=\u001b[39m sensor_depth \u001b[38;5;66;03m#torch.tensor(sensor_depths, dtype=torch.float32) / 1000.0\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m, compute_rmse(b_depths[eval_mask], gt_depths_train\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[eval_mask]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'barbara_depths' is not defined"
     ]
    }
   ],
   "source": [
    "# # Original Dense Depth Priors errors\n",
    "# near = 0.10000000149011612\n",
    "# far = 6.8439249992370605\n",
    "# b_depths = torch.tensor(barbara_depths[valid_imgs, :, :]).clamp(min=near, max=far)\n",
    "# gt_depths_train = sensor_depth #torch.tensor(sensor_depths, dtype=torch.float32) / 1000.0\n",
    "\n",
    "# print('rmse', compute_rmse(b_depths[eval_mask], gt_depths_train.squeeze(-1)[eval_mask]))\n",
    "# print('l1', torch.abs(b_depths[eval_mask] - gt_depths_train.squeeze(-1)[eval_mask]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
